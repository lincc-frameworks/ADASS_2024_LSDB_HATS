{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d26a270-2820-4f7b-8bcc-dbda854078ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Last updated: May 14, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff199ba4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Learn how to (lazy) load catalogs\n",
    "- Learn how to use those catalogs and perform crossmatching with existing `LSDB` catalogs\n",
    "- Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d7dca3-9ef8-43fb-a66c-a8caa40d8e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of lsdb is 0.2.3\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import rcParams\n",
    "import astropy.units as u\n",
    "\n",
    "# Local library-specific imports\n",
    "import lsdb\n",
    "from lsdb.core.search import BoxSearch, ConeSearch, PolygonSearch\n",
    "from hipscat.inspection import plot_pixels\n",
    "from hipscat.io.file_io import read_parquet_metadata\n",
    "\n",
    "# Jupyter-specific settings and magic commands\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# Configuration settings\n",
    "rcParams['savefig.dpi'] = 550\n",
    "rcParams['font.size'] = 20\n",
    "plt.rc('font', family='serif')\n",
    "mpl.rcParams['axes.linewidth'] = 2\n",
    "\n",
    "print(f'Version of lsdb is {lsdb.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06790ed5-de0d-4aa1-8bef-6ebc462d38f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gaia\n",
    "gaia_path = \"https://epyc.astro.washington.edu/~lincc-frameworks/hipscat_surveys/gaia_dr3/gaia\"\n",
    "\n",
    "# ZTF\n",
    "ztf_object_path = \"https://epyc.astro.washington.edu/~lincc-frameworks/hipscat_surveys/ztf/ztf_dr14/\"\n",
    "ztf_source_path = \"https://epyc.astro.washington.edu/~lincc-frameworks/hipscat_surveys/ztf/ztf_zource/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6545ef59-f273-4bd7-bc70-c24488e41804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConeSearch.__init__() got an unexpected keyword argument 'fine'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:2\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: ConeSearch.__init__() got an unexpected keyword argument 'fine'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load lite (coarse) version of Gaia DR3 (it has data from all the HEALPix intersecting with the cone)\n",
    "cone_search = ConeSearch(ra=-60, dec=20.5, radius_arcsec=1*3600)\n",
    "gaia_lite = lsdb.read_hipscat(gaia_path, columns=[\"ra\", \"dec\"], search_filter=cone_search)\n",
    "\n",
    "# Load Gaia data for the same cone (keeps only data inside the cone)\n",
    "gaia = lsdb.read_hipscat(gaia_path).cone_search(ra=-60, dec=20.5, radius_arcsec=1*3600, fine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0d32827fe4955",
   "metadata": {},
   "source": [
    "## Reading Parquet Metadata\n",
    "\n",
    "HiPSCat uses the Parquet file format to store catalogs. Parquet is a binary columnar data format, which means that information is efficiently encoded and compressed in binary format on disk, and is stored column wise in a way which allows efficiently loading only a subset of the columns. With each column, Parquet stores the column's metadata, including the column's name and data type.\n",
    "\n",
    "Parquet also supports large datasets being partitioned into multiple files that are easier to work with individually. With a partitioned dataset, like HiPSCat uses, there are metadata files at the root folder containing the partitioned files that store the combined and common metadata of each of the individual partition files metadata. Here, we can read this `_common_metadata` which includes the schema of the dataset, consisting of the column metadata for all the columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3531c7b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/fsspec/registry.py:242\u001B[0m, in \u001B[0;36mget_filesystem_class\u001B[0;34m(protocol)\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 242\u001B[0m     register_implementation(protocol, \u001B[43m_import_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbit\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mclass\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/fsspec/registry.py:277\u001B[0m, in \u001B[0;36m_import_class\u001B[0;34m(cls, minv)\u001B[0m\n\u001B[1;32m    276\u001B[0m s3 \u001B[38;5;241m=\u001B[39m mod \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3fs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 277\u001B[0m mod \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m s3 \u001B[38;5;129;01mand\u001B[39;00m mod\u001B[38;5;241m.\u001B[39m__version__\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/fsspec/implementations/http.py:9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m urlparse\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01maiohttp\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01myarl\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'aiohttp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/hipscat/io/file_io/file_pointer.py:45\u001B[0m, in \u001B[0;36mget_fs\u001B[0;34m(file_pointer, storage_options)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 45\u001B[0m     file_system \u001B[38;5;241m=\u001B[39m \u001B[43mfsspec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/fsspec/registry.py:296\u001B[0m, in \u001B[0;36mfilesystem\u001B[0;34m(protocol, **storage_options)\u001B[0m\n\u001B[1;32m    290\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124marrow_hdfs\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m protocol has been deprecated and will be \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mremoved in the future. Specify it as \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhdfs\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m    294\u001B[0m     )\n\u001B[0;32m--> 296\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mget_filesystem_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstorage_options)\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/fsspec/registry.py:244\u001B[0m, in \u001B[0;36mget_filesystem_class\u001B[0;34m(protocol)\u001B[0m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 244\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(bit[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merr\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m registry[protocol]\n",
      "\u001B[0;31mImportError\u001B[0m: HTTPFileSystem requires \"requests\" and \"aiohttp\" to be installed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mlist\u001B[39m(\u001B[43mread_parquet_metadata\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgaia_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_common_metadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mschema)[:\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/hipscat/io/file_io/file_io.py:191\u001B[0m, in \u001B[0;36mread_parquet_metadata\u001B[0;34m(file_pointer, storage_options, **kwargs)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_parquet_metadata\u001B[39m(\n\u001B[1;32m    182\u001B[0m     file_pointer: FilePointer, storage_options: Union[Dict[Any, Any], \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    183\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pq\u001B[38;5;241m.\u001B[39mFileMetaData:\n\u001B[1;32m    184\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Read FileMetaData from footer of a single Parquet file.\u001B[39;00m\n\u001B[1;32m    185\u001B[0m \n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;124;03m        **kwargs: additional arguments to be passed to pyarrow.parquet.read_metadata\u001B[39;00m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 191\u001B[0m     file_system, file_pointer \u001B[38;5;241m=\u001B[39m \u001B[43mget_fs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_pointer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfile_pointer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    193\u001B[0m     file_pointer \u001B[38;5;241m=\u001B[39m strip_leading_slash_for_pyarrow(file_pointer, protocol\u001B[38;5;241m=\u001B[39mfile_system\u001B[38;5;241m.\u001B[39mprotocol)\n\u001B[1;32m    195\u001B[0m     parquet_file \u001B[38;5;241m=\u001B[39m pq\u001B[38;5;241m.\u001B[39mread_metadata(file_pointer, filesystem\u001B[38;5;241m=\u001B[39mfile_system, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.conda/envs/tape_static/lib/python3.10/site-packages/hipscat/io/file_io/file_pointer.py:47\u001B[0m, in \u001B[0;36mget_fs\u001B[0;34m(file_pointer, storage_options)\u001B[0m\n\u001B[1;32m     45\u001B[0m     file_system \u001B[38;5;241m=\u001B[39m fsspec\u001B[38;5;241m.\u001B[39mfilesystem(protocol, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstorage_options)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m file_system, file_pointer\n",
      "\u001B[0;31mImportError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "list(read_parquet_metadata(os.path.join(gaia_path, \"_common_metadata\")).schema)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7065345d27baa23",
   "metadata": {},
   "source": [
    "## Lazy Operations\n",
    "\n",
    "When working with large datasets, there is too much data to be loaded into memory at once. To get around this, LSDB uses the HiPSCat format which partitions a catalog into HEALPix cells and works on one partition at a time. This also allows the computation to be parallelized to work on multiple partitions at once. In order to efficiently carry out pipelines of operations though, it's better to batch operations so that multiple operations can be done back to back on the same partition instead of having to load and save each partition from storage after every operation.\n",
    "\n",
    "For this reason, operations in LSDB are performed 'lazily'. This means when a catalog is read using `read_hipscat`, the actual catalog data isn't being read from storage. Instead, it only loads the metadata such as the column schema and the HEALPix structure of the partitions. When an operation like `cone_search` is called on a catalog, the data is not actually loaded and operated on when the line of code is executed. Instead, the catalog keeps track of the operations that it needs to perform so the entire pipeline can be efficiently run later. This also allows us to optimize the pipeline by only loading the partitions that are necessary. For example when performing a cone search like we do here, we only need the partitions that have data within the cone.\n",
    "\n",
    "So when we look at a catalog that has been lazy loaded we see the DataFrame without the data, just the columns and the number of partitions (including the HiPSCat index of each partition encoding which HEALPix cell the partition is in). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b33969a743777e",
   "metadata": {},
   "source": [
    "To load the data and perform the operations, call `compute()` which will load the necessary data and perform all the operations that have been called, and return a Pandas DataFrame with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaed846",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_lite_computed = gaia_lite.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_lite_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737f746828ae10e",
   "metadata": {},
   "source": [
    "## HiPSCat Partitioning\n",
    "\n",
    "To make it easier and more efficient to perform operations in parallel, HiPSCat partitions contain roughly the same number of rows. This is done by using different HEALPix pixel sizes for different parts of the sky depending on the density of sources. This means catalogs with more rows will have smaller pixels for each partition, and so will have more partitions overall. We can see this below with the ZTF object and source catalogs, where the source catalog with many more data points has more partitions to keep the size of each partition consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4d77f-c530-4a51-955e-c33c7d3439a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ztf_object = lsdb.read_hipscat(ztf_object_path, columns=[\"ra\", \"dec\"]) # ZTF Object\n",
    "ztf_source = lsdb.read_hipscat(ztf_source_path, columns=[\"ra\", \"dec\"]) # ZTF Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c297be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278461c7eab4be3",
   "metadata": {},
   "source": [
    "We can see this difference in partition pixel sizes by plotting the HEALPix pixels of the partitions in the catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b62fda-0aa5-4d1e-9a20-09775741fa03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the Pixel Density Maps for Gaia and ZTF\n",
    "\n",
    "#plot_pixels(gaia.hc_structure)\n",
    "plot_pixels(ztf_object.hc_structure)\n",
    "plot_pixels(ztf_source.hc_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60cd4b4b2a00b7",
   "metadata": {},
   "source": [
    "## The `Head` function\n",
    "\n",
    "For large operations we might want to see a small subset of the computed final data without doing the full computation. Like pandas, we can use the `head` operation to compute the first n rows of the final dataframe without computing the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0d17d-5452-4f38-ae21-d3d893854965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross-match Gaia and ZTF\n",
    "%time \n",
    "xmatch_object = gaia_lite.crossmatch(ztf_object)\n",
    "\n",
    "xmatch_object.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a45a9fceccd06",
   "metadata": {},
   "source": [
    "Here we load the ztf object catalog again, this time with all the columns that we'll need to perform the analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75601f-2fe6-4023-a530-2aa9eccf9100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# load ZTF with all columns here\n",
    "ztf_object_full = lsdb.read_hipscat(ztf_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de81488-d15d-42cd-ac03-fa12318c0c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# crossmatch ZTF + Gaia\n",
    "_all_sky_object = gaia.crossmatch(ztf_object_full).query(\n",
    "    \"nobs_g_ztf_dr14 > 50 and nobs_r_ztf_dr14 > 50 and \\\n",
    "    parallax_gaia > 0 and parallax_over_error_gaia > 5 and \\\n",
    "    teff_gspphot_gaia > 5380 and teff_gspphot_gaia < 7220 and logg_gspphot_gaia > 4.5 \\\n",
    "    and logg_gspphot_gaia < 4.72 and classprob_dsc_combmod_star_gaia > 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a712aac-3ccf-4bff-ae49-4450d626cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total = _all_sky_object.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2f381-a506-4a7f-8b80-9e882ec92bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081f008-2d73-4cfc-8f10-09c707e0079b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(total['ra_gaia'].values, total['dec_gaia'].values, color='black', s=1, label='crossmatched GAIA data')\n",
    "plt.scatter(gaia_lite_computed['ra'], gaia_lite_computed['dec'], color='blue', s=0.1, alpha=0.1, label='GAIA data')\n",
    "\n",
    "# Create a circle patch\n",
    "circle = patches.Circle((300, 20.5), 1, edgecolor='red', facecolor='lightblue', fill=False)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.gca().add_patch(circle)\n",
    "plt.xlabel('ra [deg]')\n",
    "plt.ylabel('dec [deg]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c4f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export crossmatched data to disk\n",
    "_all_sky_object.to_hipscat(base_catalog_path=\"ztf_x_gaia\", catalog_name=\"ztf_x_gaia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b59cff-dc4f-4e58-b3af-d5687fd9e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-tape_static)",
   "language": "python",
   "name": "conda-env-.conda-tape_static-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
