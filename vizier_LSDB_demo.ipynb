{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Andy Tzanidakis \\\n",
    "Last updated: May 05, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "### VizieR Query\n",
    "\n",
    "\n",
    "### Crossmatch to ZTF\n",
    "\n",
    "### Compute Time-Series Featues with `TAPE`\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "rcParams['savefig.dpi'] = 550\n",
    "rcParams['font.size'] = 20\n",
    "plt.rc('font', family='serif')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['axes.linewidth'] = 2\n",
    "\n",
    "import lsdb\n",
    "import tape\n",
    "from tape import Ensemble, ColumnMapper\n",
    "\n",
    "import dask\n",
    "dask.config.set({\"temporary-directory\" :'/epyc/ssd/users/atzanida/tmp'})\n",
    "dask.config.set({\"dataframe.shuffle-compression\": 'Snappy'})\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VizieR Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ViZier and Aladin querying \n",
    "from pyvo import registry  # version >=1.4.1 \n",
    "from mocpy import MOC\n",
    "from ipyaladin import Aladin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the catalogue name in VizieR (Gaia DR3 part 6)\n",
    "CATALOGUE = \"I/360\"\n",
    "\n",
    "catalogue_ivoid = f\"ivo://CDS.VizieR/{CATALOGUE}\"\n",
    "\n",
    "# the actual query to the registry\n",
    "voresource = registry.search(ivoid=catalogue_ivoid)[0]\n",
    "\n",
    "tables = voresource.get_tables()\n",
    "\n",
    "# We can also extract the tables names for later use\n",
    "tables_names = list(tables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Available table names: {tables_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read quickly the table description...\n",
    "voresource.describe(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first table name\n",
    "table_name_1 = \"I/360/goldf\"\n",
    "first_table_name = table_name_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_service = voresource.get_service(\"tap\")\n",
    "tap_records = voresource.get_service(\"tap\").run_sync(f'SELECT TOP 2000000  * \\\n",
    "                            FROM \"{first_table_name}\" WHERE (DE_ICRS > -30)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table0 = tap_records.to_table()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "table_df = table0.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only M-type stars\n",
    "table_df = table_df[table_df['SpType'].str.contains('M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=6, threads_per_worker=1, memory_limit='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hips_object = lsdb.from_dataframe(\n",
    "    table_df,\n",
    "    catalog_name=\"golden\",\n",
    "    catalog_type=\"object\",\n",
    "    ra_column=\"RA_ICRS\", \n",
    "    dec_column=\"DE_ICRS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hips_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hips_object.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Additional Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ZTF source table\n",
    "ztf_sources = lsdb.read_hipscat(\"/epyc/data3/hipscat/catalogs/ztf_axs/ztf_zource\")\n",
    "\n",
    "# load ZTF object table\n",
    "ztf = lsdb.read_hipscat(\"/epyc/data3/hipscat/catalogs/ztf_axs/ztf_dr14\",\n",
    "                        columns=['ps1_objid', 'nobs_r', 'nobs_g', 'ra', 'dec']) # select only Nobs (gr) bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch_golden_ztf_object = hips_object.crossmatch(ztf, n_neighbors=1, radius_arcsec=1, require_right_margin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch_golden_ztf_object.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Head of xmatch table with 250k rows for exploration\n",
    "xmatch_golden_ztf_object_comp = xmatch_golden_ztf_object.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hips_object_v2 = lsdb.from_dataframe(\n",
    "    xmatch_golden_ztf_object_comp,\n",
    "    catalog_name=\"golden\",\n",
    "    catalog_type=\"object\",\n",
    "    ra_column=\"ra_ztf_dr14\", \n",
    "    dec_column=\"dec_ztf_dr14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch_golden_ztf_object_comp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 5))\n",
    "plt.scatter(xmatch_golden_ztf_object_comp['Teff-P_golden'],\n",
    "             np.log(xmatch_golden_ztf_object_comp['Lum-F_golden']), s=1, \n",
    "             color='#28282B')\n",
    "plt.xlim(plt.xlim(3200, 4700)[::-1])\n",
    "plt.ylim(-6, -1)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel(r\"$T_{\\rm eff}$ [K]\")\n",
    "plt.ylabel(r\"$\\log_{10} L/L_\\odot$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sync source catalog data to object (xmatched) object\n",
    "_sources = hips_object_v2.join(\n",
    "    ztf_sources, left_on=\"ps1_objid_ztf_dr14\", right_on=\"ps1_objid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an Ensemble\n",
    "ens = Ensemble(client=client)\n",
    "ens.client_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnMapper Establishes which table columns map to timeseries quantities\n",
    "colmap = ColumnMapper(\n",
    "        id_col='_hipscat_index',\n",
    "        time_col='mjd',\n",
    "        flux_col='mag',\n",
    "        err_col='magerr',\n",
    "        band_col='band',\n",
    "      )\n",
    "\n",
    "ens.from_dask_dataframe(\n",
    "    source_frame=_sources._ddf,\n",
    "    object_frame=hips_object_v2._ddf,\n",
    "    column_mapper=colmap,\n",
    "    sync_tables=False, # Avoid doing an initial sync\n",
    "    sorted=True, # If the input data is already sorted by the chosen index\n",
    "    sort=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Time-Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cesium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cesium import featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = ['amplitude',\n",
    "                'percent_beyond_1_std',\n",
    "                'maximum',\n",
    "                'median',\n",
    "                'median_absolute_deviation',\n",
    "                'percent_close_to_median',\n",
    "                'minimum',\n",
    "                'skew',\n",
    "                'std',\n",
    "                'weighted_average', \n",
    "                \"flux_percentile_ratio_mid20\", \n",
    "                \"flux_percentile_ratio_mid35\",\n",
    "                \"flux_percentile_ratio_mid50\",\n",
    "                \"flux_percentile_ratio_mid65\",\n",
    "                \"flux_percentile_ratio_mid80\",\n",
    "                \"stetson_j\",\n",
    "                  \"stetson_k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(time, mag, magerr, flag, band, custom_cols=features_to_use):\n",
    "    \n",
    "    rmv = (flag == 0) & (~np.isnan(flag)) &  (band=='r') & (~np.isnan(mag)) & (~np.isnan(magerr)) & (magerr < 99) & (mag < 99)\n",
    "\n",
    "    # Removed flagged data points\n",
    "    time_, mag_, magerr_ = time[rmv], mag[rmv], magerr[rmv]\n",
    "\n",
    "    # Compute features\n",
    "    summary_ = {}\n",
    "\n",
    "    if len(time_) > 0: \n",
    "        fset_cesium = featurize.featurize_time_series(\n",
    "        times=time_,\n",
    "        values=mag_,\n",
    "        errors=magerr_,\n",
    "        features_to_use=custom_cols)\n",
    "    else:\n",
    "        fset_cesium = pd.Series(list(np.zeros(len(features_to_use))), index=custom_cols) \n",
    "    \n",
    "    return pd.Series(fset_cesium.values[0], index=custom_cols) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrame with loc and scale as meta\n",
    "my_meta = pd.DataFrame(columns=features_to_use, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# apply calc_biweight function\n",
    "calc_ = ens.batch(\n",
    "    compute_features,\n",
    "    'mjd_ztf_zource', 'mag_ztf_zource', \n",
    "    'magerr_ztf_zource', 'catflags_ztf_zource',\n",
    "    'band_ztf_zource',\n",
    "    meta=my_meta,\n",
    "    use_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ens.object.join(calc_).update_ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"dataframe.convert-string\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task 'perform_join_on-0d672da9-d883-486a-b64f-ba49306503fe' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42437. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/epyc/users/atzanida/anaconda3/envs/lsdb_demo_true/lib/python3.10/site-packages/tape/ensemble_frame.py:720\u001b[0m, in \u001b[0;36m_Frame.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mensemble \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mensemble\u001b[39m.\u001b[39m_lazy_sync_tables_from_frame(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcompute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/epyc/users/atzanida/anaconda3/envs/lsdb_demo_true/lib/python3.10/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    376\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/epyc/users/atzanida/anaconda3/envs/lsdb_demo_true/lib/python3.10/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[39mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    663\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/epyc/users/atzanida/anaconda3/envs/lsdb_demo_true/lib/python3.10/site-packages/distributed/client.py:2244\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         exc \u001b[39m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2243\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2244\u001b[0m         \u001b[39mraise\u001b[39;00m exception\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2245\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m   2246\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mskip\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKilledWorker\u001b[0m: Attempted to run task 'perform_join_on-0d672da9-d883-486a-b64f-ba49306503fe' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42437. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obj_features = calc_.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdb_demo_true",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
